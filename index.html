
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>SimilarFaces2Me - Facial Recognition App</title>
  <!-- Tailwind CSS (CDN) -->
  
  <link rel="stylesheet" href="styles/styles.css">
  <script src="scripts/app.js"></script>
</head>
<body class="bg-gray-900 text-gray-100 flex flex-col items-center min-h-screen p-4 md:p-8 font-jetbrains">
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- Main Heading -->
  <h1 class="text-3xl md:text-4xl font-bold mb-8">
    similarfaces2.me
  </h1>

  <h2 class="text-1xl md:text-4xl font-bold mb-8">
    A distributed ML inference pipeline
  </h2>
  <!-- Random Face Button -->
  <button
    id="random-image-btn"
    class="bg-gray-800 text-gray-100 px-4 mb-6 py-2 mt-4 rounded-md hover:bg-teal-400 hover:text-teal-900 transition"
  >
    Choose a random existing face
  </button>
  <!-- Upload Container -->
  <div class="flex flex-col items-center bg-gray-800 rounded-md w-62 text-center mb-8">
    <!-- Button-like label that triggers input -->
  <label
    id="upload-label"
    for="image-upload"
    class="border-2 border-dashed border-gray-500 text-gray-400 p-6 w-full rounded-md cursor-pointer 
           hover:border-teal-400 hover:text-teal-400 transition flex flex-col items-center">

    <input 
      id="image-upload"
      type="file"
      accept="image/*"
      class="hidden"
    />
    <div
      id="image-preview"
      class="flex w-48 items-center h-48 bg-gray-700 border-2 border-gray-600 rounded-md justify-center relative"
        >
          
       <div
          id="spinner"
          class="bg-gray-800 bg-opacity-10 flex items-center justify-center rounded-md hidden"
        >
          <svg
            class="animate-spin h-8 w-8 text-teal-400"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
          >
            <circle
              class="opacity-25"
              cx="12"
              cy="12"
              r="10"
              stroke="currentColor"
              stroke-width="4"
            ></circle>
            <path
              class="opacity-75"
              fill="currentColor"
              d="M4 12a8 8 0 018-8v8h8a8 8 0 11-16 0z"
            ></path>
          </svg>
        </div>
    </div>
  </label>

  </div>
<!-- Facial Analysis Results -->
<div id="facial-analysis" class="w-full max-w-xl bg-gray-800 p-4 sm:p-6 rounded-md mb-6">
  <h2 class="text-xl sm:text-2xl font-semibold mb-4">Facial Analysis</h2>
  
  <!-- Analysis Details -->
  <div class="grid grid-cols-2 sm:grid-cols-2 md:grid-cols-4 gap-4 text-gray-100">
    <div class="bg-gray-700 p-4 rounded-md text-center">
      <h3 class="text-lg font-bold mb-2">Age</h3>
      <p id="age-result" class="text-teal-400 text-xl break-words">--</p>
    </div>
    <div class="bg-gray-700 p-4 rounded-md text-center">
      <h3 class="text-lg font-bold mb-2">Gender</h3>
      <p id="gender-result" class="text-teal-400 text-xl break-words">--</p>
    </div>
    <div class="bg-gray-700 p-4 rounded-md text-center">
      <h3 class="text-lg font-bold mb-2">Emotion</h3>
      <p id="emotion-result" class="text-teal-400 text-xl break-words">--</p>
    </div>
    <div class="bg-gray-700 p-4 rounded-md text-center">
      <h3 class="text-lg font-bold mb-2">Race</h3>
      <p id="race-result" class="text-teal-400 text-xl break-words">--</p>
    </div>
  </div>
</div>

  <!-- Results Container -->
<div class="w-full max-w-xl mb-4">
  <!-- Swipe Indicator -->
  <div
  id="finger-swipe"
  class="absolute left-1/2 w-36 h-36 bg-transparent z-10 animate-swipe flex opacity-15 transform -translate-x-1/2 pointer-events-none"
>
  <svg
    clip-rule="evenodd"
    fill-rule="evenodd"
    stroke-linejoin="round"
    stroke-miterlimit="2"
    viewBox="0 0 24 24"
    xmlns="http://www.w3.org/2000/svg"
    class="fill-gray-400 pointer-events-none"
  >
    <path
      d="m10.978 14.999v3.251c0 .412-.335.75-.752.75-.188 0-.375-.071-.518-.206-1.775-1.685-4.945-4.692-6.396-6.069-.2-.189-.312-.452-.312-.725 0-.274.112-.536.312-.725 1.451-1.377 4.621-4.385 6.396-6.068.143-.136.33-.207.518-.207.417 0 .752.337.752.75v3.251h9.02c.531 0 1.002.47 1.002 1v3.998c0 .53-.471 1-1.002 1zm-1.5-7.506-4.751 4.507 4.751 4.507v-3.008h10.022v-2.998h-10.022z"
      fill-rule="nonzero"
    ></path>
  </svg>

  <svg
    xmlns="http://www.w3.org/2000/svg"
    viewBox="0 0 24 24"
    class="fill-gray-400 pointer-events-none"
  >
    <path
      d="M18.536 7.555c-1.188-.252-4.606-.904-5.536-1.088v-3.512c0-1.629-1.346-2.955-3-2.955s-3 1.326-3 2.955v7.457c-.554-.336-1.188-.621-1.838-.715-1.822-.262-3.162.94-3.162 2.498 0 .805.363 1.613 1.022 2.271 3.972 3.972 5.688 5.125 6.059 9.534h9.919v-1.748c0-5.154 3-6.031 3-10.029 0-2.448-1.061-4.157-3.464-4.668zm.357 8.022c-.821 1.483-1.838 3.319-1.891 6.423h-6.13c-.726-3.82-3.81-6.318-6.436-8.949-.688-.686-.393-1.37.442-1.373 1.263-.006 3.06 1.884 4.122 3.205v-11.928c0-.517.458-.955 1-.955s1 .438 1 .955v6.948c0 .315.256.571.572.571.314 0 .57-.256.57-.571v-.575c0-.534.49-.938 1.014-.833.398.079.686.428.686.833v1.273c0 .315.256.571.571.571s.571-.256.571-.571v-.83c0-.531.487-.932 1.008-.828.396.078.682.424.682.828v1.533c0 .315.256.571.571.571s.571-.256.571-.571v-.912c0-.523.545-.867 1.018-.646.645.305 1.166.932 1.166 2.477 0 1.355-.465 2.193-1.107 3.354z"
    ></path>
  </svg>

  <svg
    clip-rule="evenodd"
    fill-rule="evenodd"
    stroke-linejoin="round"
    stroke-miterlimit="2"
    viewBox="0 0 24 24"
    xmlns="http://www.w3.org/2000/svg"
    class="fill-gray-400 pointer-events-none"
  >
    <path
      d="m13.022 14.999v3.251c0 .412.335.75.752.75.188 0 .375-.071.518-.206 1.775-1.685 4.945-4.692 6.396-6.069.2-.189.312-.452.312-.725 0-.274-.112-.536-.312-.725-1.451-1.377-4.621-4.385-6.396-6.068-.143-.136-.33-.207-.518-.207-.417 0-.752.337-.752.75v3.251h-9.02c-.531 0-1.002.47-1.002 1v3.998c0 .53.471 1 1.002 1zm1.5-4.498v-3.008l4.751 4.507-4.751 4.507v-3.008h-10.022v-2.998z"
      fill-rule="nonzero"
    ></path>
  </svg>
</div>
<div id="scroll-container" class="relative w-full max-w-xl overflow-x-auto flex mb-8">  
  <div class="flex gap-4 mb-8">
    <div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>

    <div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>
    <div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>
  <div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>
<div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>
<div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('');"
      ></div>
    </div>
<div
      class="w-36 bg-gray-800 p-4 rounded-md text-center transition-transform transform hover:-translate-y-1 flex-shrink-0"
    >
      <div
        class="w-full h-36 bg-gray-700 rounded-md bg-center bg-cover"
        style="background-image: url('example-face3.jpg');"
      ></div>
    </div>
    <!-- Continue up to 10 cards -->
  </div>
</div>

  <!-- About Section -->
  <div class="w-full max-w-2xl bg-gray-800 p-4 sm:p-6 rounded-md leading-relaxed mb-4 sm:mb-6">    
      <h2 class="text-xl sm:text-2xl font-semibold mb-3 sm:mb-4">About</h2>    
      <p class="mb-3 sm:mb-4"> 
        This application works by using a convolutional neural network (CNN) to first detect the faces present in an image, and then extract their mathematical representations (this representation is called an embedding). When you have access to many of these embeddings, there is a natural notion of 'distance' between them. 
    </p>
    <p class="mb-3 sm:mb-4"> 
The extracted embedding is then used to search a database. A database offers a convenient way to store and access some sort of structured data. A vector database is the same, except they are purpose built around aproximate nearest neighbour algorithms. These algorithms help us take advantage of the aforementioned notion of 'distance' when searching a set of embeddings, finding one that is (hopefully) similar to yours.
    </p>
    <p class="mb-3 sm:mb-4"> 
This process and technology is similar to what a service like Netflix or Spotify could use in their recommendation systems. For example, if you like a certain song, Spotify would query its vector database of songs, and give you suggestions of songs which embeddings are located nearby. This is of course an over simplification, and their production recommendation systems are much more complicated than this. 
    </p>
  </div>

  <div class="w-full max-w-2xl bg-gray-800 p-4 sm:p-6 rounded-md leading-relaxed mb-6">    
   <h2 class="text-xl sm:text-2xl font-semibold mb-3 sm:mb-4">The Tech Stack</h2>    
      <ul class="grid grid-cols-1 sm:grid-cols-1 md:grid-cols-1 gap-4 w-4/5 max-w-5xl pb-10 w-full">
      <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-1"
        >
          <span>Go &amp gRPC</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-1" class="mt-2 p-5 hidden">
          <p class="text-gray-300">
                The 'middleware' or 'backend' of this application is written in Go. Go was chosen for its speed,
                great standard library and package manager. All service-to-service requests are through gRPC.
          </p>
        </div>
      </li>
      <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-2"
        >
          <span>Deepface &amp; Python</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-2" class="mt-2 p-5 hidden">
          <p class="text-gray-300">
                The model and preprocessing servers of this application are written in Python.
                For quick interfacing with the models, I am using the DeepFace library. In order to allow
                use of the Neuron recompiled models and adjust to the distributed design, 
                some of the the library internals had to be messsed with. Python was chosen because of
                its mature machine learning and image processing ecosystem.
          </p>
        </div>
      </li>
      <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-3"
        >
          <span>AWS</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24" stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-3" class="mt-2 p-5 hidden">
          <p class="text-gray-300 mb-6">
            This entire application is powered by Amazon Web Services. 
          </p>
          <p class="text-gray-300 mb-6">
            S3 is to serve this static website, as a model registry, and as a feature store for intermediary 
            model outputs.
          </p>
          <p class="text-gray-300 mb-6">
            EKS powers the scaling and deployments of the microservices needed to run this application. Currently there
            four services running. The 'backend' of this application acts as middleware, distributing user 
            requests to a preprocessor, facial analysis, embedding models and the vector database.
          </p>
          <p class="text-gray-300 mb-6">
            Route53 was used for the purchase of this domain, the creation of DNS records, as well as to faciliate routing to my 
            application load balancer and frontend.
          </p>
          <p class="text-gray-300 mb-6">
            The backend/middleware as well as the preprocessor services run on t3.medium instances. Any services that are doing 
            inference run on inf1.xlarge.
          </p>
        </div>
      </li>
      <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-4"
        >
          <span>Kubernetes &amp Docker</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-4" class="mt-2 p-5 hidden">
          <p class="text-gray-300 mb-6">
            Kubernetes manifests define my deployments, the resources available to the deployments, replication counts,
            which services are deployed on which node groups, and which services are visible to one another.
          </p>
          <p class="text-gray-300 mb-6">
            There are two Dockerfiles defining the containers for this application. The backend service runs on an official Golang image. The preprocessor
            and model servers are running official Amazon Deep Learning container images. These images contain drivers specific to the Inferentia architecture, 
            which are required to utilize their hardware.
          </p>
        </div>
      </li>
      <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-5"
        >
          <span>Milvus Vector Database</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-5" class="mt-2 p-5 hidden">
          <p class="text-gray-300">
            A vector database facilitates the 'similarity' search between images using embeddings. In this application, it can be 
            used to find images of a specific person in the database if the 'random' button is clicked, or to try to find faces
            that the algorithm thinks is similar to the one you upload.
          </p>
        </div>
      </li>

    <li class="bg-gray-900 p-4 rounded-md w-full hover:bg-teal-800 transition">
        <button
          class="w-full text-left focus:outline-none expandable-button flex justify-between items-center"
          aria-expanded="false"
          data-target="content-6"
        >
          <span>Neuron Compiler</span>
          <svg
            class="w-4 h-4 transform transition-transform duration-200"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M19 9l-7 7-7-7"
            ></path>
          </svg>
        </button>
        <div id="content-6" class="mt-2 p-5 hidden">
          <p class="text-gray-300">
            Because I am doing inference on inf1.xlarge instances, I also need the models in a format that the Neuron runtime can understand
            and run. The models and weights I am using must be recompiled for this. I store these precompiled models in an S3 bucket, 
            which the model servers will pull, and initialize with on startup.
          </p>
        </div>
      </li>
    </ul>
  </div>
  <div class="w-full max-w-2xl bg-gray-800 pt-4 sm:p-6 rounded-md leading-relaxed">    
   <h2 class="text-xl sm:text-2xl font-semibold mb-3 sm:mb-4">Technical Discussion</h2>    

    <h3 class="text-xl sm:text-1xl font-semibold mb-3 sm:mb-4">System Design</h3>    

    <div class="flex justify-center items-center mt-4 mb-4">
      <img 
        src="http://similarfaces2.me/similarfaces2me.jpg" 
        alt="System Design Diagram" 
        class="max-w-full h-auto border-2 border-gray-700 rounded-md"
      />
    </div>
    <p class="mb-3 sm:mb-4"> 
        Let's go over the flow of how requests are processed, and why I am using certain technologies.
    </p>
    <ol class="mb-3">
          <li>
            1. A user sends a request through the frontend. This is through simple HTTP with an image as the payload. 
            This is recieved by my Go backend, converted into jpeg (ideally, this conversion should be in my preprocessing step), and then temporarily 
            uploaded into an S3 bucket for further processing on different nodes. Facial recognition pipelines are multi-stage, meaning that multiple models 
            are used in series for these tasks. As an example; before we are 
            able to classify the gender of a face, we first need to extract a bounding box of the face from the image (in my case, such artefacts are a 
            resolution corrected image, and an extracted face...). While classification of a face IS possible
            without face extraction, it adds useless noise (information irrelevant to the task) to the classification input, which deteriorates the 
            performance of the classification model.
          </li>
          <li class="mt-4">
            2. After the image is available in my S3 'feature store', the middleware triggers a call to the preprocessing service.
            All of the internal communications is handled via gRPC. gRPC is a communications protocol specifically made for in-datacenter 
            use. It is comparable to something like REST, except it is stricter (requires interface definition files) and more performant.
          </li>
          <li class="mt-4">
            3. The preprocessing service downloads the image from S3, and then performs processing required to transform the image into a 1024x1024 format
            which my face extraction model expects. The processing applied include re-scaling, and then padding of black pixels to the desired width and height.
            The processed image is then uploaded to S3. The 1024x1024 shape is an arbitrary shape I decided on after compiling my models with the AWS Neuron compiler,
            compilation produces a model with restricted input sizing to most effectively take advantage of hardware-specific  optimizations. I had thought about 
            compiling 
            my facial detection model multiple times for different input sizes, and then using a bucketing technique (images of size A use precompiled model X,
            while images of size B use precompiled model Y...) 
            on different shapes to provide optimal detection performance for different image sizes, but I am restricted by time.
          </li>
          <li class="mt-4">
            4. After the preprocessed image is available, the middleware triggers multiple inference calls (in parallel) to the model servers. The model servers
            pull down the processed image artefacts from S3, and then run their models on them. There are currently 6 different models that run: face extraction, 
            face embedding, emotion classification, age classification, race classification and gender classification.
            However, they are inefficiently distributed across two different inference nodes because I am limited by time (and money!). Currently, 
            face extraction actually runs twice. Once in the embedding model servers, and once in the facial analysis model servers. 
            With 3 inference nodes, I would add a face extraction similar to the preprocessing step which produces an extracted face artefact, store it in S3.
            This artefact could then be pulled down for use by all the previously mentioned models, allowing higher efficiency and scalability. 
            With 6 inference nodes, every model would get its own server, and re-use the face extraction artefact.
          </li>
          <li class="mt-4">
            5. As soon as the embedding is available, it is used to query the vector database for similar faces. This database
            was filled offline, using my local machine to create embeddings. It holds over 200k queryable embeddings. The model I used for embedding was Meta/Facebook's
            Facenet512d. I chose this because it offers relatively small sized embedding vectors (dim 512), while also offering excellent discriminatory 
            performance.
            I am not self-hosting/managing milvus vectordb on EKS. I was managing it myself at the beginning of this project because I believe in doing things 
            the hard way while learning, 
            however the costs became prohibitive since the simplest deployment of Milvus requires a minimum resource commitment of 3 m6i.xlarge instances, which
            are expensive to keep up. 
            The database response does not contain an image blob, but rather a path to an S3 bucket. This is once again to reduce the sizes of my responses, 
            and push the availability portion of this project unto AWS as much as I can.
          </li>
          <li class="mt-4">
            6. The frontend recieves the response containing paths to S3 resources, facial analysis results and populates them accordingly.
          </li>
    </ol>

    <h3 class="text-xl sm:text-1xl font-semibold mb-3 sm:mb-4">Service Definitions</h3>
    <p class="mb-3 sm:mb-4"> 
        Let's go over the service definitions to solidify the function of each service.
    </p>
    <p class="mb-3 sm:mb-4"> 
        If you've never seen proto stubs, that's OK. A protobuf file allows us to have a consistent, typed interface between services communicating
        over a network. Imagine you are a software engineer working on a team that establishing a connection with a service owned by another team, written in 
        another language. Instead of having to read through an API documentation page to figure out what types are needed and how to convert them into your 
        language, you could grab the proto file that team is maintaining, and then compile it with a proto tool for your specific langugage, which will autogenerate
        client code specific to your language.
    </p>
    <p class="mb-3 sm:mb-4"> 
        1. Preprocessor
    </p>
    <div class="flex justify-center items-center mt-4 mb-4">
      <img 
        src="http://similarfaces2.me/preprocessor.png" 
        alt="Preprocessor service definition" 
        class="max-w-full h-auto border-2 border-gray-700 rounded-md"
      />
      </div>
      <p class="mb-3 sm:mb-4"> 
          1. Preprocessor
      </p>

    <h3 class="text-xl sm:text-1xl font-semibold mb-3 sm:mb-4">Scaling</h3>
    <p class="mb-3 sm:mb-4"> 
        The distributed architecture of this app was built with scaling in mind. Because each service in the pipeline is (relatively) isolated
        I am able to set up autoscaling node groups in EKS which will spin up a new backend, model server, or preprocessor whenever there is continuous load
        applied to my services (I don't do this right now because I don't want bezos to take all of my tuition money). My Kubernetes services deployments 
        and services will then adjust to these nodegroups, create service replicas and route requests to them when they are available.
    </p>
  </div>
  </div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
      const buttons = document.querySelectorAll('.expandable-button');

      buttons.forEach((button) => {
        button.addEventListener('click', () => {
          const targetId = button.getAttribute('data-target');
          const content = document.getElementById(targetId);

          // Toggle visibility of the content
          if (content.classList.contains('hidden')) {
            content.classList.remove('hidden');
            content.classList.add('block');
            button.setAttribute('aria-expanded', 'true');
            button.querySelector('svg').classList.add('rotate-180'); // Rotate arrow icon
          } else {
            content.classList.add('hidden');
            content.classList.remove('block');
            button.setAttribute('aria-expanded', 'false');
            button.querySelector('svg').classList.remove('rotate-180'); // Reset arrow rotation
          }
        });
      });
    });
  </script>
</body>
</html>

